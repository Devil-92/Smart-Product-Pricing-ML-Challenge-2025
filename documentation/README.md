Â ğŸ—ºï¸ SMART PRODUCT PRICING â€” PERFORMANCE-OPTIMIZED ROADMAP
Target: SMAPE < 40


Phase 1: Data Understanding & Exploration (Days 1â€“3)
ğŸ” 1.1 Dataset Profiling
Load train.csv and test.csv.
Inspect data types, missing values, distributions.
Log-transform target to stabilize regression:
Â 
df['log_price'] = np.log1p(df['price'])

âœ… Reduces variance and improves regression stability.

ğŸ§  1.2 Text Structure Analysis
Parse catalog_content into title, description, bullet points.
Extract metrics:

Text length (chars, words)
Unique token counts
Missing/short entries
Identify keywords for brand and quantity (ml, pack, count).
Correlate features with log-price.
ğŸ–¼ï¸ 1.3 Image Exploration
Download sample images (50â€“100).
Extract:

Resolution, brightness, entropy, aspect ratio
Missing/invalid images
Flag low-quality images.

Phase 2: Feature Engineering (Days 4â€“7)
ğŸ§¾ 2.1 Text Feature Extraction
Structured Features

Standardize:

Brand (brand_name)
Quantity (normalize to grams/ml/count)
Category from title
Binary flags: has_brand, has_quantity, has_premium_word
Linguistic Features

len_title, len_desc, title_desc_ratio
flesch_reading_ease, avg_word_len
TF-IDF â†’ SVD (100D)
Sentence embeddings (MiniLM or sentence-transformers)
âœ… High-text features: 5â€“10 point SMAPE gain.

ğŸ–¼ï¸ 2.2 Image Feature Extraction
Extract embeddings using EfficientNet or CLIP ViT:
Â 
model = timm.create_model("efficientnet_b0", pretrained=True, num_classes=0)

Normalize and reduce with PCA (64â€“128D)
Compute CLIP similarity: cosine(text_emb, image_emb)
Compute image stats: image_entropy, contrast, dominant_color
Flags: has_image, image_quality_score
âœ… Adds 5â€“8 points SMAPE improvement.

ğŸ§© 2.3 Combined / Cross-modal Features
CLIP similarity score for text-image coherence.
Elementwise fusion:
Â 
fusion_features = [text_emb - img_emb, text_emb * img_emb]

Concatenate structured + text + image features.

Phase 3: Model Development (Days 8â€“14)
âš™ï¸ 3.1 Baseline Models
Train LightGBM on structured + text features.
Use log-price as target.
Evaluate via 5-fold GroupKFold (brand/category).
âœ… SMAPE ~55â€“58

ğŸ¤– 3.2 Model Variants
Model
Input
Purpose
XGBoost
structured + text
Strong non-linear baseline
CatBoost
categorical
Brand/category handling
Ridge / Lasso
TF-IDF
Linear baseline
MLP
embeddings
Nonlinear fusion
ViT / EfficientNet
image
Vision-only baseline

ğŸ§¬ 3.3 Fusion Models
Early Fusion: concat features â†’ MLP
Intermediate Fusion: cross-attention transformer (text + image)
Late Fusion: weighted ensemble of text + image + fusion models
âœ… Fusion drops SMAPE to ~45â€“48

Phase 4: Model Optimization (Days 15â€“18)
ğŸ›ï¸ 4.1 Hyperparameter Tuning (Optuna)
Â 
params = {
'num_leaves': trial.suggest_int(20, 150),
'learning_rate': trial.suggest_float(0.005, 0.05, log=True),
'feature_fraction': trial.suggest_float(0.5, 0.9),
'bagging_fraction': trial.suggest_float(0.5, 0.9),
'lambda_l1': trial.suggest_float(0, 10),
'lambda_l2': trial.suggest_float(0, 10),
'objective': 'regression_l1'
}

âœ… 3â€“4 point SMAPE reduction.

ğŸ“Š 4.2 Cross-validation Strategy
Use GroupKFold by brand/category to reduce leakage.
Track fold-level SMAPE variance (<2%).
ğŸ§  4.3 Regularization & Loss
MAE/Huber objective
Dropout + feature subsampling (feature_fraction=0.7)
Multi-seed averaging
âœ… Smooths predictions â†’ 1â€“2 points lower SMAPE.

Phase 5: Ensembling & Stacking (Days 19â€“20)
ğŸ§© 5.1 Weighted Blending
Â 
final_pred = (
0.4 * lgb_pred +
0.3 * xgb_pred +
0.2 * cat_pred +
0.1 * fusion_pred
)

ğŸ§  5.2 Meta-Model (Stacking)
Generate OOF predictions for all models
Train LightGBM meta-learner on OOFs
âš–ï¸ 5.3 Dynamic Ensemble by Price
Price Range
Weight Strategy
Low (<200)
0.7 text + 0.3 fusion
Medium (200â€“1000)
0.5 text + 0.5 fusion
High (>1000)
0.6 image + 0.4 fusion
âœ… Reduces SMAPE by 5â€“10 points.


Phase 6: Validation & Error Analysis (Days 21â€“22)
ğŸ” 6.1 SMAPE Analysis
Fold-wise SMAPE:
Â 
smape = 200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))

Visualize by price bin, category, brand
âš ï¸ 6.2 Weak Segment Identification
Poor text â†’ rely on image
Missing image â†’ rely on text/brand cues
Outliers â†’ clip to [P5, P95]
âœ… Improves consistency & reduces SMAPE variance.

Phase 7: Final Submission (Day 23)
ğŸ 7.1 Test Predictions
Generate ensemble predictions
Inverse log:
Â 
preds = np.expm1(final_preds)

Clip to train price bounds
ğŸ“¦ 7.2 Deliverables
submission.csv
requirements.txt
model_card.md
Pipeline: train.py, predict.py

ğŸ§± Recommended Tech Stack
Type
Libraries
Core ML
PyTorch, LightGBM, XGBoost, CatBoost
Text
NLTK, spaCy, SentenceTransformers
Image
timm, OpenCV, torchvision
Optimization
Optuna
EDA
pandas, matplotlib, seaborn

ğŸš€ Target Score Progression (Optimized)
Stage
Expected SMAPE
Key Actions
Baseline LightGBM
58.6
Structured features + log-transform
+ Text & Semantic Features
50â€“53
TF-IDF, embeddings, keyword/sentiment flags
+ Image & Basic Fusion
46â€“49
EfficientNet/CLIP embeddings, PCA + concatenation
+ Dual-Tower Alignment
43â€“45
Contrastive learning / text-image cross-attention
+ Cohort-Aware Tuning
40â€“42
GroupKFold, custom loss, hyperparameter optimization
+ Two-Stage Ensemble & Post-Process
36â€“38 âœ…
Stacking meta-learner, dynamic blending, bias calibration
